# 策略推演
## 简单强化学习
基于表格的值函数方法，将所有状态和动作的价值保存到表格中，然后不断的通过结果修改
表格中的值，实现策略的改进，方法的特点是：
* 实现起来较为简单
* 算法相对比较稳定
* 比较好理解
* 对于较大的状态空间无能为力
## 深度强化学习
将强化学习中的函数关系用神经网络来拟合，例如状态到值的映射（基于值函数的），或者是
状态到动作的映射（基于策略的），这样进一步增加了强化学习的应用范围，使得其能够解决
无穷状态问题和无穷动作问题。
### DQN方法
网络层数为3层，隐藏层单元数为200，对随机不打空的随机有80%的胜率，但是最后的效果有10%左右的抖动。
对随机不打空的对手胜率稍微降低一些，而且不够稳定，网络不能设置很大。
### A3C方法