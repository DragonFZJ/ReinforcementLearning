# 策略推演
## 简单强化学习
基于表格的值函数方法，将所有状态和动作的价值保存到表格中，然后不断的通过结果修改
表格中的值，实现策略的改进，方法的特点是：
* 实现起来较为简单
* 算法相对比较稳定
* 比较好理解
* 对于较大的状态空间无能为力
## 深度强化学习
将强化学习中的函数关系用神经网络来拟合，例如状态到值的映射（基于值函数的），或者是
状态到动作的映射（基于策略的），这样进一步增加了强化学习的应用范围，使得其能够解决
无穷状态问题和无穷动作问题。
### DQN方法
网络层数为3层，隐藏层单元数为200，对随机不打空的随机有80%的胜率，但是最后的效果有10%左右的抖动。
对随机不打空的对手胜率稍微降低一些，而且不够稳定，网络不能设置很大。
修改Reward函数之后，大概有90%的胜率。
### A3C方法
利用A3C的离散化方法，输出离散化动作，动作是one-hot格式的。效果不是很好，需要进一步调试。
### 左右手互博
前面的方法是根据对手的策略（固定策略）进行学习的，但是这样得到的结果的好坏取决于对手的强弱，为了得到更好的策略需要更强的对手，
刚好可以利用自己来训练，这就是左右手互博。基本流程是，自己和自己对战，有一个自己进行学习，另一个保持原先的状态，通过不断迭代得到更强的对手。
### gym格式
利用gym格式的环境能够很好的理清思路，并且增加环境的通用性
* init() 初始化状态动作，定义空间维度以及其他元素初始化
* reset() 初始化状态
* step() 输入动作，输出下一个状态state,reward,done,info
* render() 显示环境，可以利用gym中的rendering来画图，利用Viewer类，Transform类
* windows下可以使用的玩的：
    * CartPole-v0 短时间倒立摆
    * CartPole-v1 长时间倒立摆
    * Acrobot-v1 两自由度连杆
    * MountainCar-v0  爬山车
    * MountainCarContinuous-v0  连续控制爬山车
    * Pendulum-v0 连续控制倒立摆
### pyglet